{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 9876283,
     "sourceType": "datasetVersion",
     "datasetId": 6063414
    }
   ],
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Дообучение ruBERT"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Весь следующий код был выполнен в Kaggle Notebook на GPU T4 x2, обучение на 5 эпохах заняло около\n",
    "10 часов.  \n",
    "\n",
    "Если нужно загрузить модель, то скачайте в папку models папку по ссылке https://drive.google.com/drive/folders/1-yWsVN7jqjISkW157CojpK3h_84nHuMk?usp=sharing и напишите, к примеру:  \n",
    "\n",
    "```python\n",
    "tokenizer_name = \"DeepPavlov/rubert-base-cased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(tokenizer_name)\n",
    "model_name = \"../models/rubert_model\"\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "```"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Необходимые импорты"
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd\nimport numpy as np\n\nimport torch\nfrom transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\nfrom torch.utils.data import Dataset\n\nfrom sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score",
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2024-11-12T09:28:57.277151Z",
     "iopub.execute_input": "2024-11-12T09:28:57.278130Z",
     "iopub.status.idle": "2024-11-12T09:29:15.556887Z",
     "shell.execute_reply.started": "2024-11-12T09:28:57.278085Z",
     "shell.execute_reply": "2024-11-12T09:29:15.555815Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Чтение данных и удаление лишнего столбца"
  },
  {
   "cell_type": "code",
   "source": [
    "train_path =  \"/kaggle/input/raw-data/raw_train.csv\"\n",
    "test_path = \"/kaggle/input/raw-data/raw_test.csv\"\n",
    "train = pd.read_csv(train_path)\n",
    "test = pd.read_csv(test_path)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-12T09:29:17.445847Z",
     "iopub.execute_input": "2024-11-12T09:29:17.446821Z",
     "iopub.status.idle": "2024-11-12T09:29:26.890156Z",
     "shell.execute_reply.started": "2024-11-12T09:29:17.446776Z",
     "shell.execute_reply": "2024-11-12T09:29:26.889245Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "train.drop(\"Unnamed: 0\", axis=1, inplace=True)\ntest.drop(\"Unnamed: 0\", axis=1, inplace=True)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-12T09:29:29.261278Z",
     "iopub.execute_input": "2024-11-12T09:29:29.261727Z",
     "iopub.status.idle": "2024-11-12T09:29:29.287163Z",
     "shell.execute_reply.started": "2024-11-12T09:29:29.261688Z",
     "shell.execute_reply": "2024-11-12T09:29:29.286384Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Использование GPU"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Используется устройство:\", device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Используется устройство:\", device)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Создание класса для корректного представления данных"
  },
  {
   "cell_type": "code",
   "source": "class RubertDataset(Dataset):\n    \"\"\"\n    Класс, определяющий методы для корректной трансформации сырых батчей формата DataFrame\n    в тензоры для последующей передачи в ruBERT\n    \"\"\"\n    def __init__(self, dataframe, tokenizer):\n        self.dataframe = dataframe\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        text = self.dataframe.iloc[idx]['text']\n        label = self.dataframe.iloc[idx]['sentiment']\n\n        encoding = self.tokenizer.encode_plus(\n            text,\n            max_length=400,\n            padding='max_length', # Дополнение коротких текстов \"пустыми\" токенами\n            truncation=True,      # Отсечение лишних токенов для длинных текстов\n            return_tensors='pt'\n        )\n\n        return {\n            'input_ids': encoding['input_ids'].flatten(),           # Идентификаторы токенов\n            'attention_mask': encoding['attention_mask'].flatten(), # Маска для токенов, которые надо обработать\n            'labels': torch.tensor(label, dtype=torch.long)          # Метки классов\n        }",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-12T09:29:33.486255Z",
     "iopub.execute_input": "2024-11-12T09:29:33.486669Z",
     "iopub.status.idle": "2024-11-12T09:29:33.495609Z",
     "shell.execute_reply.started": "2024-11-12T09:29:33.486630Z",
     "shell.execute_reply": "2024-11-12T09:29:33.494411Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Загрузка токенизатора и модели"
  },
  {
   "cell_type": "code",
   "source": "model_name = \"DeepPavlov/rubert-base-cased\"\ntokenizer = BertTokenizer.from_pretrained(model_name)\nmodel = BertForSequenceClassification.from_pretrained(model_name, num_labels=3).to(device)\nfor param in model.parameters(): param.data = param.data.contiguous() # надо было для корректного сохранения модели",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-12T09:29:35.945499Z",
     "iopub.execute_input": "2024-11-12T09:29:35.945884Z",
     "iopub.status.idle": "2024-11-12T09:29:41.260434Z",
     "shell.execute_reply.started": "2024-11-12T09:29:35.945848Z",
     "shell.execute_reply": "2024-11-12T09:29:41.259587Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/24.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "663bb5cef2f948d694aae1c0a3a3c35e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "vocab.txt:   0%|          | 0.00/1.65M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4e4d89775df54d48ae58bfcbee947d36"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f7f74cac6e104f62af26d06bf5e879a1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "767d918aa8c8408695c4b5e8a269a606"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "pytorch_model.bin:   0%|          | 0.00/714M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "da617e894c8b4af1877dd71a3b8b7b1d"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Определение параметров для обучения"
  },
  {
   "cell_type": "code",
   "source": [
    "train_dataset = RubertDataset(train, tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = '/kaggle/working/models/rubert_model',\n",
    "    per_device_train_batch_size=50 ,                 # Размер батча\n",
    "    num_train_epochs=5,                              # Количество эпох\n",
    "    logging_steps=50,                                # Частота вывода логов\n",
    "    save_steps=1500,                                 # Частота сохранения модели\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset\n",
    ")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-12T09:29:43.263820Z",
     "iopub.execute_input": "2024-11-12T09:29:43.264212Z",
     "iopub.status.idle": "2024-11-12T09:29:44.415821Z",
     "shell.execute_reply.started": "2024-11-12T09:29:43.264174Z",
     "shell.execute_reply": "2024-11-12T09:29:44.414840Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Обучение"
  },
  {
   "cell_type": "code",
   "source": "trainer.train()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-12T09:29:46.648550Z",
     "iopub.execute_input": "2024-11-12T09:29:46.649641Z",
     "iopub.status.idle": "2024-11-12T20:31:27.881565Z",
     "shell.execute_reply.started": "2024-11-12T09:29:46.649587Z",
     "shell.execute_reply": "2024-11-12T20:31:27.880637Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001B[34m\u001B[1mwandb\u001B[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001B[34m\u001B[1mwandb\u001B[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001B[34m\u001B[1mwandb\u001B[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001B[34m\u001B[1mwandb\u001B[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:",
     "output_type": "stream"
    },
    {
     "output_type": "stream",
     "name": "stdin",
     "text": "  ········································\n"
    },
    {
     "name": "stderr",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112855344444483, max=1.0…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cdd87b747e234ada83902b3dc9df6384"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.18.3"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20241112_092958-gcp45nj2</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/andrew-baevski-/huggingface/runs/gcp45nj2' target=\"_blank\">/kaggle/working/models/rubert_model</a></strong> to <a href='https://wandb.ai/andrew-baevski-/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/andrew-baevski-/huggingface' target=\"_blank\">https://wandb.ai/andrew-baevski-/huggingface</a>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/andrew-baevski-/huggingface/runs/gcp45nj2' target=\"_blank\">https://wandb.ai/andrew-baevski-/huggingface/runs/gcp45nj2</a>"
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='9495' max='9495' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9495/9495 11:01:20, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>0.828800</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.613900</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.563300</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.538500</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.523300</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.504900</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.504900</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.519200</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.480400</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.484100</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.495500</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.484500</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.472400</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.481900</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.470400</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.462500</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.473200</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.449300</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.455700</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.478600</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>0.444200</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.439100</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>0.440200</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.447800</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>0.433300</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.437100</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>0.443400</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.444200</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>0.443600</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.425400</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>0.439000</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.421700</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>0.432000</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.433500</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>0.431000</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.404400</td>\n    </tr>\n    <tr>\n      <td>1850</td>\n      <td>0.423400</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.416800</td>\n    </tr>\n    <tr>\n      <td>1950</td>\n      <td>0.347200</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.340500</td>\n    </tr>\n    <tr>\n      <td>2050</td>\n      <td>0.362800</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.373200</td>\n    </tr>\n    <tr>\n      <td>2150</td>\n      <td>0.341400</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.353800</td>\n    </tr>\n    <tr>\n      <td>2250</td>\n      <td>0.345100</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.353300</td>\n    </tr>\n    <tr>\n      <td>2350</td>\n      <td>0.336500</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.349500</td>\n    </tr>\n    <tr>\n      <td>2450</td>\n      <td>0.331600</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.356500</td>\n    </tr>\n    <tr>\n      <td>2550</td>\n      <td>0.340800</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>0.348300</td>\n    </tr>\n    <tr>\n      <td>2650</td>\n      <td>0.339800</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>0.340600</td>\n    </tr>\n    <tr>\n      <td>2750</td>\n      <td>0.343400</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>0.365700</td>\n    </tr>\n    <tr>\n      <td>2850</td>\n      <td>0.338600</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>0.353400</td>\n    </tr>\n    <tr>\n      <td>2950</td>\n      <td>0.348400</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.342800</td>\n    </tr>\n    <tr>\n      <td>3050</td>\n      <td>0.350100</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>0.337300</td>\n    </tr>\n    <tr>\n      <td>3150</td>\n      <td>0.340000</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>0.355900</td>\n    </tr>\n    <tr>\n      <td>3250</td>\n      <td>0.359000</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>0.355300</td>\n    </tr>\n    <tr>\n      <td>3350</td>\n      <td>0.340300</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>0.352000</td>\n    </tr>\n    <tr>\n      <td>3450</td>\n      <td>0.354200</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.351300</td>\n    </tr>\n    <tr>\n      <td>3550</td>\n      <td>0.341200</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>0.344300</td>\n    </tr>\n    <tr>\n      <td>3650</td>\n      <td>0.327900</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>0.338600</td>\n    </tr>\n    <tr>\n      <td>3750</td>\n      <td>0.343300</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>0.341000</td>\n    </tr>\n    <tr>\n      <td>3850</td>\n      <td>0.256800</td>\n    </tr>\n    <tr>\n      <td>3900</td>\n      <td>0.250600</td>\n    </tr>\n    <tr>\n      <td>3950</td>\n      <td>0.250600</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.278700</td>\n    </tr>\n    <tr>\n      <td>4050</td>\n      <td>0.261400</td>\n    </tr>\n    <tr>\n      <td>4100</td>\n      <td>0.244600</td>\n    </tr>\n    <tr>\n      <td>4150</td>\n      <td>0.245100</td>\n    </tr>\n    <tr>\n      <td>4200</td>\n      <td>0.253900</td>\n    </tr>\n    <tr>\n      <td>4250</td>\n      <td>0.247800</td>\n    </tr>\n    <tr>\n      <td>4300</td>\n      <td>0.244000</td>\n    </tr>\n    <tr>\n      <td>4350</td>\n      <td>0.257100</td>\n    </tr>\n    <tr>\n      <td>4400</td>\n      <td>0.256200</td>\n    </tr>\n    <tr>\n      <td>4450</td>\n      <td>0.258400</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.254900</td>\n    </tr>\n    <tr>\n      <td>4550</td>\n      <td>0.251300</td>\n    </tr>\n    <tr>\n      <td>4600</td>\n      <td>0.251600</td>\n    </tr>\n    <tr>\n      <td>4650</td>\n      <td>0.257900</td>\n    </tr>\n    <tr>\n      <td>4700</td>\n      <td>0.262000</td>\n    </tr>\n    <tr>\n      <td>4750</td>\n      <td>0.251300</td>\n    </tr>\n    <tr>\n      <td>4800</td>\n      <td>0.245900</td>\n    </tr>\n    <tr>\n      <td>4850</td>\n      <td>0.265200</td>\n    </tr>\n    <tr>\n      <td>4900</td>\n      <td>0.270100</td>\n    </tr>\n    <tr>\n      <td>4950</td>\n      <td>0.245900</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.251500</td>\n    </tr>\n    <tr>\n      <td>5050</td>\n      <td>0.253700</td>\n    </tr>\n    <tr>\n      <td>5100</td>\n      <td>0.260200</td>\n    </tr>\n    <tr>\n      <td>5150</td>\n      <td>0.258500</td>\n    </tr>\n    <tr>\n      <td>5200</td>\n      <td>0.259000</td>\n    </tr>\n    <tr>\n      <td>5250</td>\n      <td>0.256600</td>\n    </tr>\n    <tr>\n      <td>5300</td>\n      <td>0.248100</td>\n    </tr>\n    <tr>\n      <td>5350</td>\n      <td>0.251600</td>\n    </tr>\n    <tr>\n      <td>5400</td>\n      <td>0.252600</td>\n    </tr>\n    <tr>\n      <td>5450</td>\n      <td>0.265500</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.243400</td>\n    </tr>\n    <tr>\n      <td>5550</td>\n      <td>0.241000</td>\n    </tr>\n    <tr>\n      <td>5600</td>\n      <td>0.256500</td>\n    </tr>\n    <tr>\n      <td>5650</td>\n      <td>0.251200</td>\n    </tr>\n    <tr>\n      <td>5700</td>\n      <td>0.237100</td>\n    </tr>\n    <tr>\n      <td>5750</td>\n      <td>0.176100</td>\n    </tr>\n    <tr>\n      <td>5800</td>\n      <td>0.168000</td>\n    </tr>\n    <tr>\n      <td>5850</td>\n      <td>0.173000</td>\n    </tr>\n    <tr>\n      <td>5900</td>\n      <td>0.182500</td>\n    </tr>\n    <tr>\n      <td>5950</td>\n      <td>0.169600</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.163200</td>\n    </tr>\n    <tr>\n      <td>6050</td>\n      <td>0.180500</td>\n    </tr>\n    <tr>\n      <td>6100</td>\n      <td>0.170300</td>\n    </tr>\n    <tr>\n      <td>6150</td>\n      <td>0.175300</td>\n    </tr>\n    <tr>\n      <td>6200</td>\n      <td>0.174400</td>\n    </tr>\n    <tr>\n      <td>6250</td>\n      <td>0.164000</td>\n    </tr>\n    <tr>\n      <td>6300</td>\n      <td>0.180100</td>\n    </tr>\n    <tr>\n      <td>6350</td>\n      <td>0.179700</td>\n    </tr>\n    <tr>\n      <td>6400</td>\n      <td>0.176300</td>\n    </tr>\n    <tr>\n      <td>6450</td>\n      <td>0.171000</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.172000</td>\n    </tr>\n    <tr>\n      <td>6550</td>\n      <td>0.176000</td>\n    </tr>\n    <tr>\n      <td>6600</td>\n      <td>0.168200</td>\n    </tr>\n    <tr>\n      <td>6650</td>\n      <td>0.183400</td>\n    </tr>\n    <tr>\n      <td>6700</td>\n      <td>0.168900</td>\n    </tr>\n    <tr>\n      <td>6750</td>\n      <td>0.164200</td>\n    </tr>\n    <tr>\n      <td>6800</td>\n      <td>0.174200</td>\n    </tr>\n    <tr>\n      <td>6850</td>\n      <td>0.180600</td>\n    </tr>\n    <tr>\n      <td>6900</td>\n      <td>0.171200</td>\n    </tr>\n    <tr>\n      <td>6950</td>\n      <td>0.175700</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.176600</td>\n    </tr>\n    <tr>\n      <td>7050</td>\n      <td>0.177300</td>\n    </tr>\n    <tr>\n      <td>7100</td>\n      <td>0.171500</td>\n    </tr>\n    <tr>\n      <td>7150</td>\n      <td>0.173500</td>\n    </tr>\n    <tr>\n      <td>7200</td>\n      <td>0.171700</td>\n    </tr>\n    <tr>\n      <td>7250</td>\n      <td>0.168300</td>\n    </tr>\n    <tr>\n      <td>7300</td>\n      <td>0.182500</td>\n    </tr>\n    <tr>\n      <td>7350</td>\n      <td>0.175300</td>\n    </tr>\n    <tr>\n      <td>7400</td>\n      <td>0.181100</td>\n    </tr>\n    <tr>\n      <td>7450</td>\n      <td>0.169800</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.167500</td>\n    </tr>\n    <tr>\n      <td>7550</td>\n      <td>0.181800</td>\n    </tr>\n    <tr>\n      <td>7600</td>\n      <td>0.178000</td>\n    </tr>\n    <tr>\n      <td>7650</td>\n      <td>0.123500</td>\n    </tr>\n    <tr>\n      <td>7700</td>\n      <td>0.124500</td>\n    </tr>\n    <tr>\n      <td>7750</td>\n      <td>0.115500</td>\n    </tr>\n    <tr>\n      <td>7800</td>\n      <td>0.118500</td>\n    </tr>\n    <tr>\n      <td>7850</td>\n      <td>0.105500</td>\n    </tr>\n    <tr>\n      <td>7900</td>\n      <td>0.128500</td>\n    </tr>\n    <tr>\n      <td>7950</td>\n      <td>0.124400</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.113100</td>\n    </tr>\n    <tr>\n      <td>8050</td>\n      <td>0.121700</td>\n    </tr>\n    <tr>\n      <td>8100</td>\n      <td>0.124100</td>\n    </tr>\n    <tr>\n      <td>8150</td>\n      <td>0.135300</td>\n    </tr>\n    <tr>\n      <td>8200</td>\n      <td>0.122700</td>\n    </tr>\n    <tr>\n      <td>8250</td>\n      <td>0.113100</td>\n    </tr>\n    <tr>\n      <td>8300</td>\n      <td>0.112400</td>\n    </tr>\n    <tr>\n      <td>8350</td>\n      <td>0.126000</td>\n    </tr>\n    <tr>\n      <td>8400</td>\n      <td>0.111100</td>\n    </tr>\n    <tr>\n      <td>8450</td>\n      <td>0.117300</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>0.123900</td>\n    </tr>\n    <tr>\n      <td>8550</td>\n      <td>0.116700</td>\n    </tr>\n    <tr>\n      <td>8600</td>\n      <td>0.115800</td>\n    </tr>\n    <tr>\n      <td>8650</td>\n      <td>0.107300</td>\n    </tr>\n    <tr>\n      <td>8700</td>\n      <td>0.105700</td>\n    </tr>\n    <tr>\n      <td>8750</td>\n      <td>0.125700</td>\n    </tr>\n    <tr>\n      <td>8800</td>\n      <td>0.115800</td>\n    </tr>\n    <tr>\n      <td>8850</td>\n      <td>0.126500</td>\n    </tr>\n    <tr>\n      <td>8900</td>\n      <td>0.120200</td>\n    </tr>\n    <tr>\n      <td>8950</td>\n      <td>0.113400</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>0.122900</td>\n    </tr>\n    <tr>\n      <td>9050</td>\n      <td>0.118300</td>\n    </tr>\n    <tr>\n      <td>9100</td>\n      <td>0.122800</td>\n    </tr>\n    <tr>\n      <td>9150</td>\n      <td>0.107800</td>\n    </tr>\n    <tr>\n      <td>9200</td>\n      <td>0.110400</td>\n    </tr>\n    <tr>\n      <td>9250</td>\n      <td>0.117000</td>\n    </tr>\n    <tr>\n      <td>9300</td>\n      <td>0.122400</td>\n    </tr>\n    <tr>\n      <td>9350</td>\n      <td>0.122200</td>\n    </tr>\n    <tr>\n      <td>9400</td>\n      <td>0.107500</td>\n    </tr>\n    <tr>\n      <td>9450</td>\n      <td>0.106400</td>\n    </tr>\n  </tbody>\n</table><p>"
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n",
     "output_type": "stream"
    },
    {
     "execution_count": 8,
     "output_type": "execute_result",
     "data": {
      "text/plain": "TrainOutput(global_step=9495, training_loss=0.27381803754632755, metrics={'train_runtime': 39699.7018, 'train_samples_per_second': 23.916, 'train_steps_per_second': 0.239, 'total_flos': 1.95167460960108e+17, 'train_loss': 0.27381803754632755, 'epoch': 5.0})"
     },
     "metadata": {}
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Сохранение модели в output"
  },
  {
   "cell_type": "code",
   "source": "trainer.save_model('/kaggle/working/models/rubert_model/final')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-12T20:39:18.639187Z",
     "iopub.execute_input": "2024-11-12T20:39:18.640061Z",
     "iopub.status.idle": "2024-11-12T20:39:20.646291Z",
     "shell.execute_reply.started": "2024-11-12T20:39:18.639992Z",
     "shell.execute_reply": "2024-11-12T20:39:20.645394Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Функция для оценки метрик"
  },
  {
   "cell_type": "code",
   "source": "def evaluate_model(y_test, y_pred) -> pd.DataFrame:\n    \"\"\"\n    Оценка метрик accuracy, precision, recall, f1-score на каждом классе с последующим усреднением\n    :param y_test: тестовые таргеты\n    :param y_pred: предсказанные таргеты\n    :return: \n    \"\"\"\n\n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred, average='macro')\n    recall = recall_score(y_test, y_pred, average='macro')\n    f1 = f1_score(y_test, y_pred, average='macro')\n\n    metrics = {\n        'Метрика': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n        'Значение': [accuracy, precision, recall, f1]\n    }\n\n    df_metrics = pd.DataFrame(metrics)\n\n    return df_metrics",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-12T20:41:00.288435Z",
     "iopub.execute_input": "2024-11-12T20:41:00.288821Z",
     "iopub.status.idle": "2024-11-12T20:41:00.296240Z",
     "shell.execute_reply.started": "2024-11-12T20:41:00.288784Z",
     "shell.execute_reply": "2024-11-12T20:41:00.295313Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Предсказания и оценка на тестовых данных"
  },
  {
   "cell_type": "code",
   "source": "test_dataset = RubertDataset(test, tokenizer)\npredictions = trainer.predict(test_dataset)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-12T20:41:37.402967Z",
     "iopub.execute_input": "2024-11-12T20:41:37.403379Z",
     "iopub.status.idle": "2024-11-12T20:48:59.175984Z",
     "shell.execute_reply.started": "2024-11-12T20:41:37.403340Z",
     "shell.execute_reply": "2024-11-12T20:48:59.175183Z"
    },
    "trusted": true
   },
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": ""
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "y_pred = predictions.predictions.argmax(axis=1)\nevaluate_model(test.sentiment, y_pred)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-12T20:50:56.258281Z",
     "iopub.execute_input": "2024-11-12T20:50:56.259047Z",
     "iopub.status.idle": "2024-11-12T20:50:56.289313Z",
     "shell.execute_reply.started": "2024-11-12T20:50:56.259006Z",
     "shell.execute_reply": "2024-11-12T20:50:56.288324Z"
    },
    "trusted": true
   },
   "execution_count": 14,
   "outputs": [
    {
     "execution_count": 14,
     "output_type": "execute_result",
     "data": {
      "text/plain": "     Метрика  Значение\n0   Accuracy  0.807944\n1  Precision  0.799779\n2     Recall  0.800572\n3   F1 Score  0.800158",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Метрика</th>\n      <th>Значение</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Accuracy</td>\n      <td>0.807944</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Precision</td>\n      <td>0.799779</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Recall</td>\n      <td>0.800572</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>F1 Score</td>\n      <td>0.800158</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Пример работы с рандомными текстами"
  },
  {
   "cell_type": "code",
   "source": "example = {\n           \"text\": \n                [\n                    \"Прекрасный врач! В следующий раз обязательно запишусь к нему снова!\",\n                    \"Ужасный врач! В следующий раз к нему не приду!\",\n                    \"Нормальный врач. Возможно, приду еще. Не уверен\"\n                ],\n           \"sentiment\": \n                [\n                    1,\n                    2,\n                    0\n                ]\n          }\nexample_df = pd.DataFrame.from_dict(example)\nexample_dataset = RubertDataset(example_df, tokenizer)\ntrainer.predict(example_dataset).predictions.argmax(axis=1)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-12T21:16:58.075644Z",
     "iopub.execute_input": "2024-11-12T21:16:58.076031Z",
     "iopub.status.idle": "2024-11-12T21:16:58.237670Z",
     "shell.execute_reply.started": "2024-11-12T21:16:58.075995Z",
     "shell.execute_reply": "2024-11-12T21:16:58.236807Z"
    },
    "trusted": true
   },
   "execution_count": 53,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": ""
     },
     "metadata": {}
    },
    {
     "execution_count": 53,
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([1, 2, 0])"
     },
     "metadata": {}
    }
   ]
  }
 ]
}
