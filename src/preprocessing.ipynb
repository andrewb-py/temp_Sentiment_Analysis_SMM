{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Предобработка",
   "id": "f6db50328b0e7181"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Функции предобработки для одного документа и для всего корпуса",
   "id": "4c72558bf7b600d7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T21:54:50.045742Z",
     "start_time": "2024-10-31T21:54:46.352419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from data_load import raw_data_load # Своя функция для загрузки исходных датасетов\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.stem import SnowballStemmer\n",
    "from razdel import tokenize"
   ],
   "id": "23ebeef166064b7a",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "48b34de30290d75b",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-31T21:54:50.077100Z",
     "start_time": "2024-10-31T21:54:50.049746Z"
    }
   },
   "source": [
    "# nltk.download('stopwords')  # Будут предупреждения, если пакет уже загружен - это нормально\n",
    "russian_stopwords = set(stopwords.words('russian'))\n",
    "stemmer = SnowballStemmer(\"russian\")\n",
    "\n",
    "# Эмоционально значимые комбинации символов\n",
    "emoticon_patterns = [\n",
    "    r'\\:\\)', \n",
    "    r'\\:\\(', \n",
    "    r'\\;\\)', \n",
    "    r'\\.\\.\\.',\n",
    "    r'\\(', \n",
    "    r'\\)'\n",
    "]\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Предобработка текста со стеммингом, учетом одиноких скобок, эмоциональных комбинаций и многоточий.\n",
    "    :param text: Строка для обработки\n",
    "    :return: Строка из обработанных токенов\n",
    "    \"\"\"\n",
    "    \n",
    "    text = text.lower()\n",
    "\n",
    "    for pattern in emoticon_patterns:\n",
    "        text = re.sub(re.escape(pattern), f' {pattern} ', text)\n",
    "    \n",
    "    # Удаление ссылок, упоминаний и лишних символов\n",
    "    text = re.sub(r'http\\S+|www\\S+|@\\S+|#\\S+', '', text)\n",
    "    text = re.sub(r'\\d+|№', '', text)\n",
    "    \n",
    "    # Сохранение эмоционально значимой пунктуации и одиночных скобок\n",
    "    text = text.replace('!', ' ! ').replace('?', ' ? ')\n",
    "    \n",
    "    # Оставляем одинокие скобки, многоточие и эмоциональные комбинации\n",
    "    text = re.sub(r'[^()\\w\\s!?\\.]', '', text)  \n",
    "\n",
    "    tokens = [token.text for token in tokenize(text) if token.text.isalpha() or token in {')', '(', '!', '?', '...', '!?'}]\n",
    "\n",
    "    processed_tokens = [\n",
    "        stemmer.stem(token) \n",
    "        for token in tokens\n",
    "        if token not in russian_stopwords or token in {'не', 'нет'}\n",
    "    ]\n",
    "    \n",
    "    return ' '.join(processed_tokens)\n",
    "\n",
    "\n",
    "def preprocess(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Создание дополнительного признака в датасете - результат применения preprocess_text к тексту.\n",
    "    :param dataset: Датасет, который надо обработать\n",
    "    :return: Обработанный датасет с новым столбцом - обработанным текстом\n",
    "    \"\"\"\n",
    "\n",
    "    tqdm.pandas(desc=\"Обработка датасета\")\n",
    "\n",
    "    dataset[\"prep_text\"] = dataset[\"text\"].progress_apply(preprocess_text)\n",
    "\n",
    "    return None"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Применение функции предобработки к датасетам и их сохранение в зависимости от того, было ли это сделано ранее.",
   "id": "e1cc309877edb578"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T22:21:48.221065Z",
     "start_time": "2024-10-31T21:54:50.079103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train, test = raw_data_load()\n",
    "train.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "test.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "\n",
    "data_dir = \"../data\"\n",
    "\n",
    "prep_train_path = os.path.join(data_dir, 'prep_train.csv')\n",
    "prep_test_path = os.path.join(data_dir, 'prep_test.csv')\n",
    "\n",
    "if not os.path.exists(prep_train_path):\n",
    "    print(\"Обработка тренировочного датасета\")\n",
    "    preprocess(train)\n",
    "    test.dropna(subset=[\"prep_text\"], inplace=True)\n",
    "    train.to_csv(prep_train_path, index=False)  # Сохранение обработанных данных\n",
    "    print(f\"Обработанные тренировочные данные сохранены\")\n",
    "\n",
    "if not os.path.exists(prep_test_path):\n",
    "    print(\"Обработка тестового датасета\")\n",
    "    preprocess(test)\n",
    "    test.dropna(subset=[\"prep_text\"], inplace=True)\n",
    "    test.to_csv(prep_test_path, index=False)  # Сохранение обработанных данных\n",
    "    print(f\"Обработанные тестовые данные сохранены\")"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обработка тренировочного датасета\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Обработка датасета:   0%|          | 0/189891 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "06228dae47eb4f08bed7e670fa2ca559"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обработанные тренировочные данные сохранены\n",
      "Обработка тестового датасета\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Обработка датасета:   0%|          | 0/21098 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9c79385919234eee87b0479e79d5d002"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обработанные тестовые данные сохранены\n"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
